大数据时代的思维变革 02：不是精确性，而是混杂性允许不精确
<img src="http://p1.pstatp.com/large/40090002d711740f7a23" img_width="1280" img_height="851" alt="大数据时代的思维变革 02：不是精确性，而是混杂性允许不精确" inline="0">在越来越多的情况下，使用所有可获取的数据变得更为可能，但为此也要付出一定的代价。数据量的大幅增加会造成结果的不准确，与此同时，一些错误的数据也会混进数据库。然而，重点是我们能够努力避免这些问题。我们从不认为这些问题是无法避免的，而且也正在学会接受它们。这就是由“小数据”到“大数据”的重要转变之一。对“小数据”而言，最基本、最重要的要求就是减少错误，保证质量。因为收集的信息量比较少，所以我们必须确保记录下来的数据尽量精确。无论是观察天体的位置还是观测显微镜下物体的大小，为了使结果更加准确，很多科学家都致力于优化测量的工具。在采样的时候，对精确度的要求就更高更苛刻了。因为收集信息的有限意味着细微的错误会被放大，甚至有可能影响整个结果的准确『性』。历史上很多时候，人们会把通过测量世界来征服世界视为最大的成就。事实上，对精确度的高要求始于13世纪中期的欧洲。那时候，天文学家和学者对时间、空间的研究采取了比以往更为精确的量化方式，用历史学家阿尔弗雷德·克罗斯比（alfred crosby）的话来说就是“测量现实”。我们研究一个现象，是因为我们相信我们能够理解它。后来，测量方法逐渐被运用到科学观察、解释方法中，体现为一种进行量化研究、记录，并呈现可重复结果的能力。罗德·凯文（lord kelvin）曾说过：“测量就是认知。”这已成为一条至理名言。培根也曾说过：“知识就是力量。”同时，很多数学家以及后来的精算师和会计师都发展了可以准确收集、记录和管理数据的方法。19世纪，科技率先发展起来的法国开发了一套能准确计量时间、空间单位的系统，并逐渐成为其他国家普遍采用的标准，这套系统还为后来国际公认的测量条约奠定了基础，成为测量时代的巅峰。仅半个世纪之后，20世纪20年代，量子力学的发现永远粉碎了“测量臻于至善”的幻梦。然而，在物理学这个小圈子以外的一些测量工程师和科学家仍沉湎在完美测量的梦中。随着理『性』学科，如数学和统计学逐渐影响到商业领域，商业界更加崇尚这种思想。然而，在不断涌现的新情况里，允许不精确的出现已经成为一个新的亮点，而非缺点。因为放松了容错的标准，人们掌握的数据也多了起来，还可以利用这些数据做更多新的事情。这样就不是大量数据优于少量数据那么简单了，而是大量数据创造了更好的结果。同时，我们需要与各种各样的混『乱』做斗争。混『乱』，简单地说就是随着数据的增加，错误率也会相应增加。所以，如果桥梁的压力数据量增加1000倍的话，其中的部分读数就可能是错误的，而且随着读数量的增加，错误率可能也会继续增加。在整合来源不同的各类信息的时候，因为它们通常不完全一致，所以也会加大混『乱』程度。例如，与服务器处理投诉时的数据进行比较，用语音识别系统识别某个呼叫中心接到的投诉会产生一个不太准确的结果，但也是有助于我们把握整个事情的大致情况的。混『乱』还可以指格式的不一致『性』，因为要达到格式一致，就需要在进行数据处理之前仔细地清洗数据，而这在大数据背景下很难做到。“大数据”专家帕堤尔（d.j. patil）指出，i.b.m.、t.j. watson labs、international business machines都可以用来指代ibm，甚至可能有成千上万种方法称呼ibm。当然，在萃取或处理数据的时候，混『乱』也会发生。因为在进行数据转化的时候，我们是在把它变成另外的事物。比如，我们在对twitter的信息进行情感分析来预测好莱坞票房的时候，就会出现一定的混『乱』。其实，混『乱』的起源和类型本来就是一团『乱』麻。假设你要测量一个葡萄园的温度，但是整个葡萄园只有一个温度测量仪，那你就必须确保这个测试仪是精确的而且能够一直工作。反过来，如果每100棵葡萄树就有一个测量仪，有些测试的数据可能会是错误的，也可能会更加混『乱』，但众多的读数合起来就可以提供一个更加准确的结果。因为这里面包含了更多的数据，而它提供的价值不仅能抵消掉错误数据造成的影响，还能提供更多的额外价值。现在想想增加读数频率的这个事情。如果每隔一分钟就测量一下温度，我们至少还能够保证测量结果是按照时间有序排列的。如果变成每分钟测量十次甚至百次的话，不仅读数可能出错，连时间先后都可能搞混掉。试想，如果信息在网络中流动，那么一条记录很可能在传输过程中被延迟，在其到达的时候已经没有意义了，甚至干脆在奔涌的信息洪流中彻底『迷』失。虽然我们得到的信息不再那么准确，但收集到的数量庞大的信息让我们放弃严格精确的选择变得更为划算。在第一个例子里，我们为了获得更广泛的数据而牺牲了精确『性』，也因此看到了很多如若不然无法被关注到的细节。在第二个例子里，我们为了高频率而放弃了精确『性』，结果观察到了一些本可能被错过的变化。虽然如果我们能够下足够多的工夫，这些错误是可以避免的，但在很多情况下，与致力于避免错误相比，对错误的包容会带给我们更多好处。为了扩大规模，我们接受适量错误的存在。正如技术咨询公司forrester所认为的，有时得到2加2约等于3.9的结果，也很不错了。当然，数据不可能完全错误，但为了了解大致的发展趋势，我们愿意对精确『性』做出一些让步。我们可以在大量数据对计算机其他领域进步的重要『性』上看到类似的变化。我们都知道，如摩尔定律所预测的，过去一段时间里计算机的数据处理能力得到了很大的提高。摩尔定律认为，每块芯片上晶体管的数量每两年就会翻一倍。这使得电脑运行更快速了，存储空间更大了。大家没有意识到的是，驱动各类系统的算法也进步了——美国总统科技顾问委员会的报告显示，在很多领域这些算法带来的进步还要胜过芯片的进步。然而，社会从“大数据”中所能得到的，并非来自运行更快的芯片或更好的算法，而是更多的数据。由于象棋的规则家喻户晓，且走子限制良多，在过去的几十年里，象棋算法的变化很小。计算机象棋程序总是步步为赢是由于对残局掌握得更好了，而之所以能做到这一点也只是因为往系统里加入了更多的数据。实际上，当棋盘上只剩下六枚棋子或更少的时候，这个残局得到了全面地分析，并且接下来所有可能的走法（样本=总体）都被制入了一个庞大的数据表格。这个数据表格如果不压缩的话，会有一太字节那么多。所以，计算机在这些重要的象棋残局中表现得完美无缺和不可战胜。<img src="http://p1.pstatp.com/large/40060002c248bfdc3ad4" img_width="1280" img_height="853" alt="大数据时代的思维变革 02：不是精确性，而是混杂性允许不精确" inline="0">大数据在多大程度上优于算法这个问题在自然语言处理上表现得很明显（这是关于计算机如何学习和领悟我们在日常生活中使用语言的学科方向）。在2000年的时候，微软研究中心的米歇尔·班科（michele banko）和埃里克·布里尔（eric bill）一直在寻求改进word程序中语法检查的方法。但是他们不能确定是努力改进现有的算法、研发新的方法，还是添加更加细腻精致的特点更有效。所以，在实施这些措施之前，他们决定往现有的算法中添加更多的数据，看看会有什么不同的变化。很多对计算机学习算法的研究都建立在百万字左右的语料库基础上。最后，他们决定往4种常见的算法中逐渐添加数据，先是一千万字，再到一亿字，最后到十亿。结果有点令人吃惊。他们发现，随着数据的增多，4种算法的表现都大幅提高了。当数据只有500万的时候，有一种简单的算法表现得很差，但数据达10亿的时候，它变成了表现最好的，准确率从原来的75%提高到了95%以上。与之相反地，在少量数据情况下运行得最好的算法，当加入更多的数据时，也会像其他的算法一样有所提高，但是却变成了在大量数据条件下运行得最不好的。它的准确率会从86%提高到94%。后来，班科和布里尔在他们发表的研究论文中写到，“如此一来，我们得重新衡量一下更多的人力物力是应该消耗在算法发展上还是在语料库发展上。”大数据的简单算法比小数据的复杂算法更有效所以，数据多比少好，更多数据比算法系统更智能还要重要。那么，混『乱』呢？在班科和布里尔开始研究数据几年后，微软的最大竞争对手，谷歌，也开始更大规模地对这些问题进行探讨。谷歌用的是上万亿的语料库，而不是十亿的。谷歌做这类研究不是因为语法检查，而是为了解决翻译这个更棘手的难题。20世纪40年代，电脑由真空管制成，要占据整个房间这么大的空间。而机器翻译也只是计算机开发人员的一个想法。在冷战时期，美国掌握了大量关于苏联的各种资料，但缺少翻译这些资料的人手。所以，计算机翻译也成了亟须解决的问题。最初，计算机研发人员打算将语法规则和双语词典结合在一起。1954年，ibm以计算机中的250个词语和六条语法规则为基础，将60个俄语词组翻译成了英语，结果振奋人心。ibm701通过穿孔卡片读取了“mipyeryedaye mmislyi posryedstvom ryechyi”这句话，并且将其译成了“我们通过语言来交流思想”。在庆祝这个成就的发布会上，一篇报道就有提到，这60句话翻译得很流畅。这个程序的指挥官利昂·多斯特尔特（leon dostert）表示，他相信“在三五年后，机器翻译将会变得很成熟”。事实证明，计算机翻译最初的成功误导了人们。1966年，一群机器翻译的研究人员意识到，翻译比他们想象的更困难，他们不得不承认他们的失败。机器翻译不能只是让电脑熟悉常用规则，还必须教会电脑处理特殊的语言情况。毕竟，翻译不仅仅只是记忆和复述，也涉及选词，而明确地教会电脑这些非常不现实。法语中的“bonjour”就一定是“早上好”吗？有没有可能是“日安”、“你好”或者“喂”？事实上都有可能——这需要视情况而定。在20世纪80年代后期，ibm的研发人员提出了一个新的想法。与单纯教给计算机语言规则和词汇相比，他们试图让计算机自己估算一个词或一个词组适合于用来翻译另一种语言中的一个词和词组的可能『性』，然后再决定某个词和词组在另一种语言中的对等词和词组。20世纪90年代，ibm的这个candide项目花费了大概十年的时间，将大约有300万句之多的加拿大议会资料译成了英语和法语并出版。由于是官方文件，翻译的标准就非常高。用那个时候的标准来看，数据量非常之庞大。统计机器学习从诞生之日起，就聪明地把翻译的挑战变成了一个数学问题，而这似乎很有效！计算机翻译在短时间内就提高了很多。然而，在这次飞跃之后，ibm公司尽管投入了很多资金，但取得的成效不大。最终，ibm公司停止了这个项目。无所不包的谷歌翻译系统2006年，谷歌公司也开始涉足机器翻译。这被当作实现“收集全世界的数据资源，并让人人都可享受这些资源”这个目标的一个步骤。谷歌翻译开始利用一个更大更繁杂的数据库，也就是全球的互联网，而不再只利用两种语言之间的文本翻译。谷歌翻译系统为了训练计算机，会吸收它能找到的所有翻译。它会从各种各样语言的公司网站上去寻找联合国和欧洲委员会这些国际组织发布的官方文件和报告的译本。它甚至会吸收速读项目中的书籍翻译。谷歌翻译部的负责人弗朗兹·奥齐（franz och）是机器翻译界的权威，他指出，“谷歌的翻译系统不会像candide一样只是仔细地翻译300万句话，它会掌握用不同语言翻译的质量参差不齐的数十亿页的文档。”不考虑翻译质量的话，上万亿的语料库就相当于950亿句英语。尽管其输入源很混『乱』，但较其他翻译系统而言，谷歌的翻译质量相对而言还是最好的，而且可翻译的内容更多。到2012年年中，谷歌数据库涵盖了60多种语言，甚至能够接受14种语言的语音输入，并有很流利的对等翻译。之所以能做到这些，是因为它将语言视为能够判别可能『性』的数据，而不是语言本身。如果要将印度语译成加泰罗尼亚语，谷歌就会把英语作为中介语言。因为在翻译的时候它能适当增减词汇，所以谷歌的翻译比其他系统的翻译灵活很多。谷歌的翻译之所以更好并不是因为它拥有一个更好的算法机制。和微软的班科和布里尔一样，这是因为谷歌翻译增加了很多各种各样的数据。从谷歌的例子来看，它之所以能比ibm的candide系统多利用成千上万的数据，是因为它接受了有错误的数据。2006年，谷歌发布的上万亿的语料库，就是来自于互联网的一些废弃内容。这就是“训练集”，可以正确地推算出英语词汇搭配在一起的可能『性』。20世纪60年代，拥有百万英语单词的语料库——布朗语料库算得上这个领域的开创者，而如今谷歌的这个语料库则是一个质的突破，后者使用庞大的数据库使得自然语言处理这一方向取得了飞跃式的发展。自然语言处理能力是语音识别系统和计算机翻译的基础。彼得·诺维格（peter norvig），谷歌公司人工智能方面的专家，和他的同事在一篇题为《数据的非理『性』效果》（the unreasonable effectiveness of data）的文章中写道，“大数据基础上的简单算法比小数据基础上的复杂算法更加有效。”诺维格和他同事就指出，混杂是关键。“从某种意义上，谷歌的语料库是布朗语料库的一个退步。因为谷歌语料库的内容来自于未经过滤的网页内容，所以会包含一些不完整的句子、拼写错误、语法错误以及其他各种错误。况且，它也没有详细的人工纠错后的注解。但是，谷歌语料库是布朗语料库的好几百万倍大，这样的优势完全压倒了缺点。”纷繁的数据越多越好传统的样本分析师们很难容忍错误数据的存在，因为他们一生都在研究如何防止和避免错误的出现。在收集样本的时候，统计学家会用一整套的策略来减少错误发生的概率。在结果公布之前，他们也会测试样本是否存在潜在的系统『性』偏差。这些策略包括根据协议或通过受过专门训练的专家来采集样本。但是，即使只是少量的数据，这些规避错误的策略实施起来还是耗费巨大。尤其是当我们收集所有数据的时候，这就行不通了。不仅是因为耗费巨大，还因为在大规模的基础上保持数据收集标准的一致『性』不太现实。就算是不让人们进行沟通，也不能解决这个问题。大数据时代要求我们重新审视精确『性』的优劣。如果将传统的思维模式运用于数字化、网络化的21世纪，就会错过重要的信息。执『迷』于精确『性』是信息缺乏时代和模拟时代的产物。在那个信息贫乏的时代，任意一个数据点的测量情况都对结果至关重要。所以，我们需要确保每个数据的精确『性』，才不会导致分析结果的偏差。混杂『性』，不是竭力避免，而是标准途径确切地说，在许多技术和社会领域，我们更倾向于纷繁混杂。我们来看看内容分类方面的情况。几个世纪以来，人们一直用分类法和索引法来帮助自己存储和检索数据资源。这样的分级系统通常都不完善——各位读者没有忘记图书馆卡片目录给你们带来的痛苦回忆吧？在“小数据”范围内，这些方法就很有效，但一旦把数据规模增加好几个数量级，这些预设一切都各就各位的系统就会崩溃。<img src="http://p3.pstatp.com/large/40060002c2157152a88b" img_width="1280" img_height="959" alt="大数据时代的思维变革 02：不是精确性，而是混杂性允许不精确" inline="0">相片分享网站flickr在2011年拥有来自大概1亿用户的60亿张照片。根据预先设定好的分类来标注每张照片就没有意义了。难道真会有人为他的照片取名“像希特勒一样的猫”吗？恰恰相反，清楚的分类被更混『乱』却更灵活的机制所取代。这些机制才能适应改变着的世界。当我们上传照片到flickr网站的时候，我们会给照片添加标签。也就是说，我们会使用一组文本标签来编组和搜索这些资源。人们用自己的方式创造和使用标签，所以它是没有标准、没有预先设定的排列和分类，也没有我们必须遵守的类别的。任何人都可以输入新的标签，标签内容事实上就成为网络资源的分类标准。标签被广泛地应用于facebook、博客等社交网络上。因为它们的存在，互联网上的资源变得更加容易找到，特别是像图片、视频和音乐这些无法用关键词搜索的非文本类资源。当然，有时人们错标的标签会导致资源编组的不准确，这会让习惯了精确『性』的人们很痛苦。但是，我们用来编组照片集的混『乱』方法给我们带来了很多好处。比如，我们拥有了更加丰富的标签内容，同时能更深更广地获得各种照片。我们可以通过合并多个搜索标签来过滤我们需要寻找的照片，这在以前是无法完成的。我们添加标签时所固带的不准确『性』从某种意义上说明我们能够接受世界的纷繁复杂。这是对更加精确系统的一种对抗。这些精确的系统试图让我们接受一个世界贫乏而规整的惨相——假装世间万物都是整齐地排列的。而事实上现实是纷繁复杂的，天地间存在的事物也远远多于系统所设想的。互联网上最火的网址都表明，它们欣赏不精确而不会假装精确。当一个人在网站上见到一个facebook的“喜欢”按钮时，可以看到有多少其他人也在点击。当数量不多时，会显示像“63”这种精确的数字。当数量很大时，则只会显示近似值，比方说“4000”。这并不代表系统不知道正确的数据是多少，只是当数量规模变大的时候，确切的数量已经不那么重要了。另外，数据更新得非常快，甚至在刚刚显示出来的时候可能就已经过时了。所以，同样的原理适用于时间的显示。谷歌的gmail邮箱会确切标注在很短时间内收到的信件，比方说“11分钟之前”。但是，对于已经收到一段时间的信件，则会标注如“两个小时之前”这种不太确切的时间信息。2000年以来，商务智能和分析软件领域的技术供应商们一直承诺给客户“一个唯一真理”。执行官们用这个词组并没有讽刺的意思，现在也依然有技术供应商这样说。他们说这个词组的意思就是，每个使用该公司信息技术系统的人都能利用同样的数据资源，这样市场部和营销部的人员们就不需要再在会议开始前争论，到底是谁掌握了正确的客户和销售数据了。这个想法就是说，如果他们知道的数据是一致的，那么他们的利益也会更一致。但是，“一个唯一的真理”这种想法已经彻底被改变了。现在不但出现了一种新的认识，即“一个唯一的真理”的存在是不可能的，而且追求这个唯一的真理是对注意力的分散。要想获得大规模数据带来的好处，混『乱』应该是一种标准途径，而不应该是竭力避免的。我们甚至发现，不精确已经渗入了数据库设计这个最不能容忍错误的领域。传统的数据库引擎要求数据高度精确和准确排列。数据不是单纯地被存储，它往往被划分为包含“域”的记录，每个域都包含了特定种类和特定长度信息。比方说，某个数值域是7个数字长，一个1000万或者更大的数值就无法被记录。一个人想在某个记录手机号码的域中输入一串汉字是“不被允许”的。想要被允许也可以，需要改变数据库结构才可以。现在，我们依然在和电脑以及智能手机上的这些限制进行斗争，比如软件可能拒绝记录我们输入的数据。索引是事先就设定好了的，这也就限制了人们的搜索。增加一个新的索引往往既消耗时间，又惹人讨厌，因为需要改变底层的设计。传统的关系数据库是为数据稀缺的时代设计的，所以能够也需要仔细策划。在那个时代，人们遭遇到的问题无比清晰，所以数据库被设计用来有效地回答这些问题，但是，这种数据存储和分析的方法越来越和现实相冲突。我们现在拥有各种各样、参差不齐的海量数据，很少有数据完全符合预先设定的数据种类。而且，我们想要的数据回答的问题，也只有在我们收集和处理数据的过程中才会知道。<img src="http://p3.pstatp.com/large/40080002d803031b34b3" img_width="1280" img_height="853" alt="大数据时代的思维变革 02：不是精确性，而是混杂性允许不精确" inline="0">